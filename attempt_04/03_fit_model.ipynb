{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c02ece8-26f0-4630-a7e1-ba50767212c9",
   "metadata": {},
   "source": [
    "# Create text representations using BERT\n",
    "* Load the train/validate/tests datasets\n",
    "* Compute topic distributions applying BERT\n",
    "\n",
    "https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027fe2ba-d575-4d4f-b5f6-cb6bf688ec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobv/atroncos/arxiv_exp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from bertopic import BERTopic\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e21b100b-561a-477e-9d85-ee8acd2450c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84a6740-9bfd-415a-8aa9-d4eae34d4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(name):\n",
    "    # make sure the authors_parsed column contains arrays of str, 1 str per author name\n",
    "    # e.g. ['Bognár, Zs., ', 'Sódor, Á., ', 'Clark, I. R., ', 'Kawaler, S. D., ']\n",
    "    return pd.read_csv(\n",
    "        os.path.join(DATA_PATH, name), \n",
    "        index_col=0, \n",
    "        converters={\"authors_parsed\": lambda x:[entry.replace(\"'\", '').strip(\"[]\") for entry in x.split(\"', '\")]}\n",
    "    )\n",
    "\n",
    "train_df = load_df('arxiv_train.csv')\n",
    "validate_df = load_df('arxiv_validate.csv')\n",
    "test_df = load_df('arxiv_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d4eda6-a9a3-4e3c-ae87-39f5ac35404c",
   "metadata": {},
   "source": [
    "## Scikit-Learn Embeddings\n",
    "Create a text representation using Scikit-Learn embeddings and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c452c531-030b-43c2-8fbd-935129f2052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37268144-371b-4a55-b2ae-2bd11ed0f225",
   "metadata": {},
   "source": [
    "Fit the model on the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24b167c-ce1f-427c-9aca-f10b7253a845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n",
      "CPU times: user 4min 10s, sys: 1min 38s, total: 5min 48s\n",
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    TruncatedSVD(100)  # dimensionality reduction, like PCA\n",
    ")\n",
    "topic_model_BERT_scikit = BERTopic(embedding_model=pipe).fit(train_df['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167c5e5-7436-455f-8b53-f21ef01862d0",
   "metadata": {},
   "source": [
    "Compute a soft topic distribution, where each document can have multiple topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29c5a33c-50a5-444e-aa56-c3218e48aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_distr(docs, topic_model):\n",
    "    distr, _ = topic_model.approximate_distribution(docs)\n",
    "    return distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125926c3-63c3-4a72-82ed-41306c36c757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 15.8 s, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_topic_distr = topic_distr(train_df['abstract'], topic_model_BERT_scikit)\n",
    "validate_topic_distr = topic_distr(validate_df['abstract'], topic_model_BERT_scikit)\n",
    "test_topic_distr = topic_distr(test_df['abstract'], topic_model_BERT_scikit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6853fa55-4fdb-4d38-b1bb-406f7fd9ba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic distribution matrix for the train dataset has 45265 distributions (one for each document) for 284 topics.\n",
      "The topic distribution matrix for the validate dataset has 22632 distributions (one for each document) for 284 topics.\n",
      "The topic distribution matrix for the test dataset has 22633 distributions (one for each document) for 284 topics.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The topic distribution matrix for the train dataset has {train_topic_distr.shape[0]} distributions (one for each document) for {train_topic_distr.shape[1]} topics.\")\n",
    "print(f\"The topic distribution matrix for the validate dataset has {validate_topic_distr.shape[0]} distributions (one for each document) for {validate_topic_distr.shape[1]} topics.\")\n",
    "print(f\"The topic distribution matrix for the test dataset has {test_topic_distr.shape[0]} distributions (one for each document) for {test_topic_distr.shape[1]} topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7b24afe-d73a-41ea-90a1-096c8e13c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples:\n",
    "\n",
    "# Display the topic distribution for document 0 in validate dataset\n",
    "# validate_topic_distr[0]\n",
    "\n",
    "# Display info for topics\n",
    "# topic_model_BERT_scikit.get_topic_info()\n",
    "\n",
    "# Display keywords for topic\n",
    "# topic_model_BERT_scikit.get_topic(177)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37065d3a-e235-4088-a905-b0cb8c523756",
   "metadata": {},
   "source": [
    "### Save the topic model and distribution matrices\n",
    "These model and matrices are too large for Git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a89d16c-9bbd-430a-a341-4ed8b4ac8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'topic_model_BERT_scikit.pickle'), 'wb') as handle:\n",
    "    pickle.dump(topic_model_BERT_scikit, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'train_topic_distr_BERT_scikit.pickle'), 'wb') as handle:\n",
    "    pickle.dump(train_topic_distr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'validate_topic_distr_BERT_scikit.pickle'), 'wb') as handle:\n",
    "    pickle.dump(validate_topic_distr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'test_topic_distr_BERT_scikit.pickle'), 'wb') as handle:\n",
    "    pickle.dump(test_topic_distr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c816c87d-b06d-49bd-b0a8-fbb98823e58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
