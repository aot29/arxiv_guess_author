{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e3c58b-c280-4a9c-86fc-2f48a54e7b0c",
   "metadata": {},
   "source": [
    "# Test: Using a BERT topic model to guess authors of a paper submitted for blind peer review\n",
    "\n",
    "*Goal*\n",
    "> **Guess the authors of a paper submitted for blind peer review.**\n",
    "\n",
    "A guess should be made based solely on: \n",
    "* text (abstract)\n",
    "* references to other papers \n",
    "* year of submission\n",
    "* target journal, i.e. the journal that asked for the review\n",
    "No other information is available at the time of submission, notably the article classification used by the journal is not always available at the time of submission. However, a broad category can be deduced from the abstract or the target journal, e.g. \"Physics\" or \"Computer Science\".\n",
    "\n",
    "*Assumptions*\n",
    "> 1. **An individual author writes about similar topics within a short time range.**\n",
    "> 2. **The author of the paper submitted for review is actively involved in research at the present time.**\n",
    "> 3. **Individual authors can be identified by name.**\n",
    "\n",
    "This is essential while choosing the dataset to conduct the experiment. Data was obtained by querying arXiv for articles using these inclusion criteria:\n",
    "* Articles have to be in the broad category \"Physics\" (the largest category on arXiv),\n",
    "* Articles have to be published in the last 2 years. This time span of 2 years is arbitrary and is an attempt to balance the assumptions 1 and 2. \n",
    "\n",
    "*Hypothesis*\n",
    "> **It should be possible to compute a \"topic distribution\" for individual authors by extracting the topics from all their articles combined. Based on these topic distributions, it should be possible to compute the \"topic distance\" between two authors. Guessing the author of an article then could be accomplished by extracting the topics from the newly submitted article and finding the authors with the most similar topic distributions, i.e. with the shortest topic distance to the article.**\n",
    "\n",
    "Extracting topics can be accomplished by applying the LDA algorithm [4] to the abstracts of the articles. The best number of topics can be obtained by reducing the perplexity measure of the model. LDA and perplexity need not be implemented, as the GENSIM library [5] provides a well tested implementation. Topic distances are Euclidean distances in the topic space. \n",
    "\n",
    "### Workflow\n",
    "\n",
    "#### Data acquisition ([01_process_snapshot](./01_process_snapshot.ipynb))\n",
    "A data dump was obtained from ArXiv [1]. The data contains metadata and abstracts for 2.486.206 articles. The data was augmented with columns for:\n",
    "* year of submission to arXiv\n",
    "* binary columns for broad categories (\"Computer Science\", \"Economics\", \"Mathematics\", \"Physics\", etc.), an article can be classified within multiple categories.\n",
    "\n",
    "#### Data preparation ([02_prepare_data](./02_prepare_data.ipynb))\n",
    "The data was queried for all articles on Physics (the largest category on arXiv), submitted since beginning 2023 (assuming that researchers are active in the present). The data has 90.530 articles, written by 246.443 authors. The data was split in train (50%), validate (25%) and test (25%) datasets. \n",
    "\n",
    "#### Fit topic model ([03_fit_topic_model](./03_fit_topic_model.ipynb))\n",
    "Compute topic distributions applying BERT. These are soft topic distribution, where each document can have multiple topics. The result is a matrix where each row is a document and columns are the probability that the document belongs in a topic.\n",
    "\n",
    "#### \n",
    "\n",
    "### References\n",
    "* [1] arXiv.org submitters. (2024). arXiv Dataset [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/7548853\n",
    "* [2] [The gensim preprocessing module](https://github.com/piskvorky/gensim/blob/develop/gensim/parsing/preprocessing.py)\n",
    "* [3] [WordNetLemmatizer in NLTK library 3.8](https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet)\n",
    "* [4] [Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "* [5] [GENSIM library 4.3](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "* [6] Maier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., ... & Adam, S. (2021). Applying LDA topic modeling in communication research: Toward a valid and reliable methodology. In Computational methods for communication science (pp. 13-38). Routledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49901ebd-be71-4257-9b6d-a5fe9013dd21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
