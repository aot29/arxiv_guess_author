{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97146dde-de0d-4fc3-8482-d195fd7a1c33",
   "metadata": {},
   "source": [
    "# Test\n",
    "* Load the tokenized abstracts of the test dataset.\n",
    "* Extract the topics of each article applying the topic model, load the article metadata and merge with the topics into one data frame. \n",
    "* Find the authors closest to the articles in the topic space.\n",
    "* Compute the probability that the closest author is one of the authors of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3ffccf1-c753-4e8a-81cc-547cd1087a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2081b5e4-a045-4cc7-a748-d51fd74bc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'\n",
    "MODELS_PATH = '../models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23784fe5-5c6c-4882-a901-ea4b91105bd9",
   "metadata": {},
   "source": [
    "## Load the tokenized abstracts of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c258dd1-ab89-48a2-9011-567d1b30a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary\n",
    "with open(os.path.join(DATA_PATH, 'dictionary.pickle'), 'rb') as handle:\n",
    "    dictionary = tokenized_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dacfd7f5-a081-49ce-ad89-e5eb8d7fbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized abstracts\n",
    "def load_tokenized_dataset(file_name):\n",
    "    path = os.path.join(DATA_PATH, file_name)\n",
    "    with open(path, 'rb') as handle:\n",
    "        tokenized_dataset = pickle.load(handle)\n",
    "    return tokenized_dataset\n",
    "\n",
    "corpus_test = load_tokenized_dataset(\"corpus_test.pickle\")\n",
    "corpus_validate = load_tokenized_dataset(\"corpus_validate.pickle\")\n",
    "corpus_train = load_tokenized_dataset(\"corpus_train.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690062b3-10ce-4cd2-a078-610b9b207583",
   "metadata": {},
   "source": [
    "## Extract the topics of each article applying the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3b4378f-fe62-4c24-82b6-664de2042d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the topic model\n",
    "with open(os.path.join(MODELS_PATH, 'topic_model.pickle'), 'rb') as handle:\n",
    "    topic_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e552f396-feb1-416e-a19b-b6acc95c9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(topic_model, corpus):\n",
    "    \"\"\"\n",
    "    Returns a list of pandas Series object of tuples. \n",
    "    Each tuple is a topic number and the topic probability for this entry in the corpus.\n",
    "    Example: \n",
    "        [[(0, 0.22764261), (4, 0.14444388), (5, 0.62411755)],\n",
    "         [(1, 0.024827635), (2, 0.3290665), (3, 0.6061594), (5, 0.03431195)],\n",
    "         [(0, 0.06239689), (3, 0.03924617), (5, 0.8926314)],\n",
    "         [(3, 0.09784623), (5, 0.89414346)],...\n",
    "        ...]\n",
    "    If for a given entry, the topic's probability is 0, then the topic is not included in the Series for this entry.\n",
    "    \"\"\"\n",
    "    topic_details_list = []\n",
    "    for row in topic_model[corpus]:\n",
    "        topic_details_list.append(row)\n",
    "    return topic_details_list\n",
    "\n",
    "def get_topic_dataframe(topic_model, corpus):\n",
    "    \"\"\"\n",
    "    Returns a data frame with a column for each topic in the topic model.\n",
    "    Each row stands for an entry in the corpus, each value for the probability of thos topic for this entry.\n",
    "    If for a given entry, the topic's probability is 0, the the value in the entry's column corresponding to the topic is also 0.\n",
    "    Example:\n",
    "             \t0 \t1 \t2 \t3 \t4 \t5\n",
    "        0 \t0.227641 \t0.000000 \t0.000000 \t0.000000 \t0.144445 \t0.624118\n",
    "        1 \t0.000000 \t0.024817 \t0.329062 \t0.606161 \t0.000000 \t0.034325\n",
    "        2 \t0.062392 \t0.000000 \t0.000000 \t0.039281 \t0.000000 \t0.892601\n",
    "        3 \t0.000000 \t0.000000 \t0.000000 \t0.097728 \t0.000000 \t0.894262\n",
    "    \"\"\"\n",
    "    topic_details = get_topic_details(topic_model, corpus)\n",
    "    topics_entries = []  # topics for all entries\n",
    "    num_topics = len(topic_model.get_topics())  # number of topics in the model\n",
    "    for row in topic_details:\n",
    "        topics_entry = [0] * num_topics\n",
    "        for entry in row:  # all topic probabilities for this entry\n",
    "            topic_num = entry[0]  # the topic number\n",
    "            topic_prob = entry[1]  # the topic probability            \n",
    "            topics_entry[topic_num] = topic_prob\n",
    "        topics_entries.append(topics_entry)\n",
    "    return pd.DataFrame(topics_entries, columns=range(0, num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35ec5a79-954b-4697-9929-19c7b8eb4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to assign topics probabilities to all articles in test data set\n",
    "topics_test_df = get_topic_dataframe(topic_model, corpus_test)\n",
    "# Use the model to assign topics probabilities to all articles in validate data set\n",
    "topics_validate_df = get_topic_dataframe(topic_model, corpus_validate)\n",
    "# Use the model to assign topics probabilities to all articles in train data set\n",
    "topics_train_df = get_topic_dataframe(topic_model, corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f6a2f-978e-4783-8525-f017be16260b",
   "metadata": {},
   "source": [
    "## Load the article metadata and merge with the topics into one data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f8b14-3820-49fd-8a8b-d5b7fedcf278",
   "metadata": {},
   "source": [
    "* load the article metadata\n",
    "* Add names to the topic columns, i.e. topic_0, ...\n",
    "* merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90157ff5-911f-4b55-8541-6bb5e4ca3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(name):\n",
    "    return pd.read_csv(\n",
    "        os.path.join(DATA_PATH, name), \n",
    "        index_col=0, \n",
    "        converters={\"authors_parsed\": lambda x:[entry.replace(\"\\'\", '').replace('\"', '').strip(\"[]\") for entry in x.split('\", \"')]})\n",
    "\n",
    "validate_df = load_df('arxiv_validate.csv')\n",
    "test_df = load_df('arxiv_test.csv')\n",
    "train_df = load_df('arxiv_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6afbff9b-b3c6-4cba-a798-80ad0e01d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(topic_model.get_topics())\n",
    "\n",
    "for l in [topics_test_df, topics_validate_df, topics_train_df]:\n",
    "        l.columns = [\"topic_%d\"%n for n in range(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9226f322-4409-4f42-b7d7-cd3222c48191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(article_df, topics_df):\n",
    "    merged = article_df.reset_index(drop=True).join(topics_df.reset_index(drop=True))\n",
    "    return merged\n",
    "\n",
    "test_df = merge_df(test_df, topics_test_df)\n",
    "validate_df = merge_df(validate_df, topics_validate_df)\n",
    "train_df = merge_df(train_df, topics_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfafce6-f671-4bf6-aa41-5487004709b6",
   "metadata": {},
   "source": [
    "## Find the authors closest to the articles in the topic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5185a76-c04e-49f1-b38d-5629978dd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the topics by author\n",
    "authors_validate_df = pd.read_csv(os.path.join(DATA_PATH, 'validate_topics_authors.csv'), index_col=0)\n",
    "authors_test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_topics_authors.csv'), index_col=0)\n",
    "authors_train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_topics_authors.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d992bde4-f100-4027-9c72-50cb3f94edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def topic_distance(topics1, topics2):\n",
    "    \"\"\"\n",
    "    topic1, topic2: numpy.ndarray, representing the probability that an article is about a topic. An article can have multiple topics.\n",
    "    Example:\n",
    "        0    0.000000\n",
    "        1    0.000000\n",
    "        2    0.000000\n",
    "        3    0.000000\n",
    "        4    0.000000\n",
    "        5    0.992668\n",
    "    topic1 and topic2 must be the same length.\n",
    "    Topics were assigned to articles in 03_assign_topics\n",
    "    \"\"\"\n",
    "    dist = np.linalg.norm(topics1 - topics2)  # euclidean distance, L2 norm is default\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f9d00-469f-4d5b-96a1-24f0296925ac",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Pick a random article from the validate data set, find the nearest author in validate dataset.\n",
    "Expect to guess one of the authors in the majority of cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "086bb61c-324e-44c5-988e-84b321c4e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_author(article, authors_df):\n",
    "    \"\"\"\n",
    "    Guess the author of article within authors_df.\n",
    "    \n",
    "    article: pandas.DataFrame data for a single article\n",
    "    authors_df: pandas.DataFrame data for all authors\n",
    "    \"\"\"\n",
    "    # numpy array with topics probabilities for the article\n",
    "    topics1 = np.array(article[[\"topic_%d\"%n for n in range(num_topics)]])\n",
    "    # numpy array of topics of all authors\n",
    "    topics_authors_np = authors_df[authors_df.columns.drop('author')].to_numpy() \n",
    "    # compute distances between article topics and topics of all authors\n",
    "    distances = [topic_distance(topics1, topics2) for topics2 in topics_authors_np]\n",
    "    # find the closest author\n",
    "    closest_author = authors_df.iloc[distances.index(min(distances))].author\n",
    "    return closest_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4f74af2e-a74a-4347-898b-562746cc3f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original article \"Revealing the spatial nature of sublattice symmetry\" authors are: ['Xiao, Rong, ', 'Zhao, Y. X., ']\n",
      "The closest author is: Xiao, Rong, \n"
     ]
    }
   ],
   "source": [
    "# pick a random paper\n",
    "rnd_article = validate_df.sample()\n",
    "closest_author = guess_author(rnd_article, authors_validate_df)\n",
    "\n",
    "# check\n",
    "print(f\"The original article \\\"{rnd_article.title.iloc[0]}\\\" authors are: {rnd_article.authors_parsed.iloc[0]}\")\n",
    "print(f\"The closest author is: {closest_author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9826ac39-a792-4881-8920-d41f7c55197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 1 / 22632\n",
      "Processing article 1001 / 22632\n",
      "Processing article 2001 / 22632\n",
      "Processing article 3001 / 22632\n",
      "Processing article 4001 / 22632\n",
      "Processing article 5001 / 22632\n",
      "Processing article 6001 / 22632\n",
      "Processing article 7001 / 22632\n",
      "Processing article 8001 / 22632\n",
      "Processing article 9001 / 22632\n",
      "Processing article 10001 / 22632\n",
      "Processing article 11001 / 22632\n",
      "Processing article 12001 / 22632\n",
      "Processing article 13001 / 22632\n",
      "Processing article 14001 / 22632\n",
      "Processing article 15001 / 22632\n",
      "Processing article 16001 / 22632\n",
      "Processing article 17001 / 22632\n",
      "Processing article 18001 / 22632\n",
      "Processing article 19001 / 22632\n",
      "Processing article 20001 / 22632\n",
      "Processing article 21001 / 22632\n",
      "Processing article 22001 / 22632\n",
      "Guessed the authors of 18387/22632 articles correctly, i.e. 81.24 %\n",
      "CPU times: user 1h 9min 39s, sys: 16.4 s, total: 1h 9min 55s\n",
      "Wall time: 1h 9min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "correct = 0\n",
    "counter = 0\n",
    "for _, article in validate_df.iterrows():\n",
    "    if counter % 1000 == 0: print(f\"Processing article {counter+1} / {validate_df.shape[0]}\")\n",
    "    closest_author = guess_author(article, authors_validate_df)\n",
    "    if closest_author in article.authors_parsed:\n",
    "        correct += 1\n",
    "    counter += 1\n",
    "print(f\"Guessed the authors of {correct}/{validate_df.shape[0]} articles correctly, i.e. {correct/validate_df.shape[0]*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0060938-137f-4ac9-8abf-0fa6b3e952ea",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "Pick a random article from the validate data set, by an author who is also in the train dataset. Find the nearest author in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8871659f-e6a0-485e-8177-af19f738da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validate dataset has 101576, the train dataset has 157213, and 50935 are in both datasets.\n"
     ]
    }
   ],
   "source": [
    "# Percentage of authors in validate dataset, that are also present in train dataset\n",
    "authors_validate_set = set(authors_validate_df.author)\n",
    "authors_train_set = set(authors_train_df.author)\n",
    "authors_intersection = authors_validate_set.intersection(authors_train_set)\n",
    "print(f\"The validate dataset has {len(authors_validate_set)}, the train dataset has {len(authors_train_set)}, and {len(authors_intersection)} are in both datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f66d0834-8c56-4910-8270-01443052ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19018 out of 22632 articles in the validate dataset were written by authors present in the validate and the train datasets\n",
      "i.e. 84.03 %\n"
     ]
    }
   ],
   "source": [
    "# get articles in validate dataset, written by authors who are in both datasets\n",
    "intersection_ids = []\n",
    "for i, row in validate_df.iterrows():\n",
    "    for author in row.authors_parsed:\n",
    "        # if at least one author is in both datasets, keep the article id\n",
    "        if author in authors_intersection:\n",
    "            intersection_ids.append(row.id)\n",
    "            break\n",
    "\n",
    "idx = [article_id in intersection_ids for article_id in validate_df.id]\n",
    "intersection_articles_df = validate_df[idx]\n",
    "\n",
    "print(f\"{intersection_articles_df.shape[0]} out of {validate_df.shape[0]} articles in the validate dataset were written by authors present in the validate and the train datasets\")\n",
    "print(f\"i.e. {intersection_articles_df.shape[0]/validate_df.shape[0]*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "848a9683-bdd0-469f-91f8-de1172c4f823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original article \"Junctions, Edge Modes, and $G_2$-Holonomy Orbifolds\" authors are: ['Acharya, Bobby Samir, ', 'Del Zotto, Michele, ', 'Heckman, Jonathan J., ', 'Hubner, Max, ', 'Torres, Ethan, ']\n",
      "The closest author is: Jardim, I. C., \n"
     ]
    }
   ],
   "source": [
    "# pick a random paper in validate dataset, written by authors who are in both datasets\n",
    "rnd_article = intersection_articles_df.sample()\n",
    "# Find the nearest author in train dataset.\n",
    "closest_author = guess_author(rnd_article, authors_train_df)\n",
    "\n",
    "# check\n",
    "print(f\"The original article \\\"{rnd_article.title.iloc[0]}\\\" authors are: {rnd_article.authors_parsed.iloc[0]}\")\n",
    "print(f\"The closest author is: {closest_author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0c9aa92a-b688-430f-a3df-4504301b7e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 1 / 19018\n",
      "Processing article 1001 / 19018\n",
      "Processing article 2001 / 19018\n",
      "Processing article 3001 / 19018\n",
      "Processing article 4001 / 19018\n",
      "Processing article 5001 / 19018\n",
      "Processing article 6001 / 19018\n",
      "Processing article 7001 / 19018\n",
      "Processing article 8001 / 19018\n",
      "Processing article 9001 / 19018\n",
      "Processing article 10001 / 19018\n",
      "Processing article 11001 / 19018\n",
      "Processing article 12001 / 19018\n",
      "Processing article 13001 / 19018\n",
      "Processing article 14001 / 19018\n",
      "Processing article 15001 / 19018\n",
      "Processing article 16001 / 19018\n",
      "Processing article 17001 / 19018\n",
      "Processing article 18001 / 19018\n",
      "Processing article 19001 / 19018\n",
      "Guessed the authors of 10/22632 articles correctly, i.e. 0.04 %\n",
      "CPU times: user 1h 32min, sys: 16.3 s, total: 1h 32min 17s\n",
      "Wall time: 1h 32min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "correct = 0\n",
    "counter = 0\n",
    "for _, article in intersection_articles_df.iterrows():\n",
    "    if counter % 1000 == 0: print(f\"Processing article {counter+1} / {intersection_articles_df.shape[0]}\")\n",
    "    closest_author = guess_author(article, authors_train_df)\n",
    "    if closest_author in article.authors_parsed:\n",
    "        correct += 1\n",
    "    counter += 1\n",
    "print(f\"Guessed the authors of {correct}/{validate_df.shape[0]} articles correctly, i.e. {correct/validate_df.shape[0]*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06abf2-62ec-4388-8bfd-a09366e0b0be",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a41eb7b1-ec1b-40e8-9f63-fea0fe7576e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics for the authors in the train dataset\n",
    "# the topic_authors dataframes have an extra column for author names, so use the 5 first columns only\n",
    "topics_authors_np = train_topics_authors.iloc[:, 0:5].to_numpy()  \n",
    "\n",
    "def guess_author(article_topics):\n",
    "    # numpy array with topics probabilities for the article\n",
    "    topics1 = np.array(article)\n",
    "    # compute distance from article topics to all authors topics in the train dataset\n",
    "    distances = [topic_distance(topics1, topics2) for topics2 in topics_authors_np]\n",
    "    # find closest author\n",
    "    closest_author = train_topics_authors.iloc[distances.index(min(distances))]\n",
    "    return closest_author\n",
    "\n",
    "def guess_authors(article_topics_df, article_df):\n",
    "    closest_authors = []\n",
    "    for i, article in article_topics_df.iterrows():\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processing article {i+1}/{article_topics_df.shape[0]}\")\n",
    "        closest_authors.append(guess_author(article))\n",
    "    return closest_authors\n",
    "\n",
    "def check_guess(article_df, closest_authors):\n",
    "    for i, article in article_df.iterrows():\n",
    "        # check if correct\n",
    "        check.append(closest_author[i].author in article.authors_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "512998f9-d6a2-4d9b-a164-d2b98ba7217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 236 ms, sys: 23.8 ms, total: 260 ms\n",
      "Wall time: 245 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                  0.119213\n",
       "1                       0.0\n",
       "2                       0.0\n",
       "3                       0.0\n",
       "4                  0.870843\n",
       "author    Snegirev, A. V., \n",
       "Name: 46206, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "guess_author(topics_train_df.iloc[0])\n",
    "\n",
    "#guesses = guess_authors(topics_train_df, train_df)\n",
    "#check = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f010137-49b4-48f3-a2f0-39d1c7cb039b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
