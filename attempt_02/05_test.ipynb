{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97146dde-de0d-4fc3-8482-d195fd7a1c33",
   "metadata": {},
   "source": [
    "# Test\n",
    "* Load the tokenized abstracts of the test dataset.\n",
    "* Extract the topics of each article applying the topic model.\n",
    "* Find the authors closest to the articles in the topic space.\n",
    "* Compute the probability that the closest author is one of the authors of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ffccf1-c753-4e8a-81cc-547cd1087a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2081b5e4-a045-4cc7-a748-d51fd74bc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'\n",
    "MODELS_PATH = '../models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23784fe5-5c6c-4882-a901-ea4b91105bd9",
   "metadata": {},
   "source": [
    "## Load the tokenized abstracts of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c258dd1-ab89-48a2-9011-567d1b30a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary\n",
    "with open(os.path.join(DATA_PATH, 'dictionary.pickle'), 'rb') as handle:\n",
    "    dictionary = tokenized_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dacfd7f5-a081-49ce-ad89-e5eb8d7fbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized abstracts\n",
    "def load_tokenized_dataset(file_name):\n",
    "    path = os.path.join(DATA_PATH, file_name)\n",
    "    with open(path, 'rb') as handle:\n",
    "        tokenized_dataset = pickle.load(handle)\n",
    "    return tokenized_dataset\n",
    "\n",
    "corpus_test = load_tokenized_dataset(\"corpus_test.pickle\")\n",
    "corpus_validate = load_tokenized_dataset(\"corpus_validate.pickle\")\n",
    "corpus_train = load_tokenized_dataset(\"corpus_train.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690062b3-10ce-4cd2-a078-610b9b207583",
   "metadata": {},
   "source": [
    "## Extract the topics of each article applying the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b4378f-fe62-4c24-82b6-664de2042d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the topic model\n",
    "with open(os.path.join(MODELS_PATH, 'topic_model.pickle'), 'rb') as handle:\n",
    "    topic_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e552f396-feb1-416e-a19b-b6acc95c9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(topic_model, corpus):\n",
    "    \"\"\"\n",
    "    Returns a list of pandas Series object of tuples. \n",
    "    Each tuple is a topic number and the topic probability for this entry in the corpus.\n",
    "    Example: \n",
    "        [[(0, 0.22764261), (4, 0.14444388), (5, 0.62411755)],\n",
    "         [(1, 0.024827635), (2, 0.3290665), (3, 0.6061594), (5, 0.03431195)],\n",
    "         [(0, 0.06239689), (3, 0.03924617), (5, 0.8926314)],\n",
    "         [(3, 0.09784623), (5, 0.89414346)],...\n",
    "        ...]\n",
    "    If for a given entry, the topic's probability is 0, then the topic is not included in the Series for this entry.\n",
    "    \"\"\"\n",
    "    topic_details_list = []\n",
    "    for row in topic_model[corpus]:\n",
    "        topic_details_list.append(row)\n",
    "    return topic_details_list\n",
    "\n",
    "def get_topic_dataframe(topic_model, corpus):\n",
    "    \"\"\"\n",
    "    Returns a data frame with a column for each topic in the topic model.\n",
    "    Each row stands for an entry in the corpus, each value for the probability of thos topic for this entry.\n",
    "    If for a given entry, the topic's probability is 0, the the value in the entry's column corresponding to the topic is also 0.\n",
    "    Example:\n",
    "             \t0 \t1 \t2 \t3 \t4 \t5\n",
    "        0 \t0.227641 \t0.000000 \t0.000000 \t0.000000 \t0.144445 \t0.624118\n",
    "        1 \t0.000000 \t0.024817 \t0.329062 \t0.606161 \t0.000000 \t0.034325\n",
    "        2 \t0.062392 \t0.000000 \t0.000000 \t0.039281 \t0.000000 \t0.892601\n",
    "        3 \t0.000000 \t0.000000 \t0.000000 \t0.097728 \t0.000000 \t0.894262\n",
    "    \"\"\"\n",
    "    topic_details = get_topic_details(topic_model, corpus)\n",
    "    topics_entries = []  # topics for all entries\n",
    "    num_topics = len(topic_model.get_topics())  # number of topics in the model\n",
    "    for row in topic_details:\n",
    "        topics_entry = [0] * num_topics\n",
    "        for entry in row:  # all topic probabilities for this entry\n",
    "            topic_num = entry[0]  # the topic number\n",
    "            topic_prob = entry[1]  # the topic probability            \n",
    "            topics_entry[topic_num] = topic_prob\n",
    "        topics_entries.append(topics_entry)\n",
    "    return pd.DataFrame(topics_entries, columns=range(0, num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "35ec5a79-954b-4697-9929-19c7b8eb4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to assign topics probabilities to all articles in test data set\n",
    "topics_test_df = get_topic_dataframe(topic_model, corpus_test)\n",
    "# Use the model to assign topics probabilities to all articles in validate data set\n",
    "topics_validate_df = get_topic_dataframe(topic_model, corpus_validate)\n",
    "# Use the model to assign topics probabilities to all articles in train data set\n",
    "topics_train_df = get_topic_dataframe(topic_model, corpus_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ab8397be-eccc-4eda-9b4e-063515301f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the topics\n",
    "topics_test_df.to_csv(os.path.join(DATA_PATH, 'topics_test.csv'))\n",
    "topics_validate_df.to_csv(os.path.join(DATA_PATH, 'topics_validate.csv'))\n",
    "topics_train_df.to_csv(os.path.join(DATA_PATH, 'topics_train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfafce6-f671-4bf6-aa41-5487004709b6",
   "metadata": {},
   "source": [
    "## Find the authors closest to the articles in the topic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5185a76-c04e-49f1-b38d-5629978dd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the topics by author\n",
    "validate_topics_authors = pd.read_csv(os.path.join(DATA_PATH, 'validate_topics_authors.csv'), index_col=0)\n",
    "test_topics_authors = pd.read_csv(os.path.join(DATA_PATH, 'test_topics_authors.csv'), index_col=0)\n",
    "train_topics_authors = pd.read_csv(os.path.join(DATA_PATH, 'train_topics_authors.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "90157ff5-911f-4b55-8541-6bb5e4ca3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the article metadata\n",
    "def load_df(name):\n",
    "    return pd.read_csv(\n",
    "        os.path.join(DATA_PATH, name), \n",
    "        index_col=0, \n",
    "        converters={\"authors_parsed\": lambda x:[entry.replace(\"\\'\", '').replace('\"', '').strip(\"[]\") for entry in x.split('\", \"')]})\n",
    "\n",
    "validate_df = load_df('arxiv_validate.csv')\n",
    "test_df = load_df('arxiv_test.csv')\n",
    "train_df = load_df('arxiv_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d992bde4-f100-4027-9c72-50cb3f94edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def topic_distance(topics1, topics2):\n",
    "    \"\"\"\n",
    "    topic1, topic2: numpy.ndarray, representing the probability that an article is about a topic. An article can have multiple topics.\n",
    "    Example:\n",
    "        0    0.000000\n",
    "        1    0.000000\n",
    "        2    0.000000\n",
    "        3    0.000000\n",
    "        4    0.000000\n",
    "        5    0.992668\n",
    "    topic1 and topic2 must be the same length.\n",
    "    Topics were assigned to articles in 03_assign_topics\n",
    "    \"\"\"\n",
    "    dist = np.linalg.norm(topics1 - topics2)  # euclidean distance, L2 norm is default\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f9d00-469f-4d5b-96a1-24f0296925ac",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Pick a random article from the validate data set, find the nearest author in validate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8703798c-3a11-438a-9f6a-8db281696043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 0 ns, total: 2 μs\n",
      "Wall time: 4.29 μs\n",
      "Computing 101576 distances\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# pick a random paper\n",
    "rnd_article = topics_validate_df.sample()\n",
    "# numpy array with topics probabilities for the article\n",
    "topics1 = np.array(rnd_article.values[0])\n",
    "\n",
    "print(f\"Computing {validate_topics_authors.shape[0]} distances\")\n",
    "validate_topics_authors_np = validate_topics_authors.iloc[:, 0:5].to_numpy()  # the topic_authors dataframes have an extra column for author names\n",
    "distances = [topic_distance(topics1, topics2) for topics2 in validate_topics_authors_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "407d4a06-af44-4d1d-9ba6-86d1e3704545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original article \"White-light superflare and long-term activity of the nearby M7 type\n",
      "  binary EI~Cnc observed with GWAC system\" authors are: ['Li, Hua-Li, ', 'Wang, Jing, ', 'Xin, Li-Ping, ', 'Bai, Jian-Ying, ', 'Han, Xu-Hui, ', 'Cai, Hong-Bo, ', 'Huang, Lei, ', 'Lu, Xiao-Meng, ', 'Qiu, Yu-Lei, ', 'Wu, Chao, ', 'Li, Guang-Wei, ', 'Deng, Jing-Song, ', 'Xu, Da-Wei, ', 'Yang, Yuan-Gui, ', 'Wang, Xiang-Gao, ', 'Liang, En-Wei, ', 'Wei, Jian-Yan, ']\n"
     ]
    }
   ],
   "source": [
    "original_article = validate_df.iloc[rnd_article.index[0]]\n",
    "print(f\"The original article \\\"{original_article.title}\\\" authors are: {original_article.authors_parsed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "367de7b6-905d-4a32-938d-55d4a70fc224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest author is: Cai, Hong-Bo, \n"
     ]
    }
   ],
   "source": [
    "closest_author = validate_topics_authors.iloc[distances.index(min(distances))]\n",
    "print(f\"The closest author is: {closest_author.author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9826ac39-a792-4881-8920-d41f7c55197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_author.author in original_article.authors_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06abf2-62ec-4388-8bfd-a09366e0b0be",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a41eb7b1-ec1b-40e8-9f63-fea0fe7576e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics for the authors in the train dataset\n",
    "# the topic_authors dataframes have an extra column for author names, so use the 5 first columns only\n",
    "topics_authors_np = train_topics_authors.iloc[:, 0:5].to_numpy()  \n",
    "\n",
    "def guess_author(article_topics):\n",
    "    # numpy array with topics probabilities for the article\n",
    "    topics1 = np.array(article)\n",
    "    # compute distance from article topics to all authors topics in the train dataset\n",
    "    distances = [topic_distance(topics1, topics2) for topics2 in topics_authors_np]\n",
    "    # find closest author\n",
    "    closest_author = train_topics_authors.iloc[distances.index(min(distances))]\n",
    "    return closest_author\n",
    "\n",
    "def guess_authors(article_topics_df, article_df):\n",
    "    closest_authors = []\n",
    "    for i, article in article_topics_df.iterrows():\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processing article {i+1}/{article_topics_df.shape[0]}\")\n",
    "        closest_authors.append(guess_author(article))\n",
    "    return closest_authors\n",
    "\n",
    "def check_guess(article_df, closest_authors):\n",
    "    for i, article in article_df.iterrows():\n",
    "        # check if correct\n",
    "        check.append(closest_author[i].author in article.authors_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "512998f9-d6a2-4d9b-a164-d2b98ba7217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 236 ms, sys: 23.8 ms, total: 260 ms\n",
      "Wall time: 245 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                  0.119213\n",
       "1                       0.0\n",
       "2                       0.0\n",
       "3                       0.0\n",
       "4                  0.870843\n",
       "author    Snegirev, A. V., \n",
       "Name: 46206, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "guess_author(topics_train_df.iloc[0])\n",
    "\n",
    "#guesses = guess_authors(topics_train_df, train_df)\n",
    "#check = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1e26bb7e-8722-467c-87d4-5dfe97afb68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ekici, Cagin, ',\n",
       " 'Yu, Yonghe, ',\n",
       " 'Adcock, Jeremy C., ',\n",
       " 'Muthali, Alif Laila, ',\n",
       " 'Zahidy, Mujtaba, ',\n",
       " 'Tan, Heyun, ',\n",
       " 'Lin, Zhongjin, ',\n",
       " 'Li, Hao, ',\n",
       " 'Oxenløwe, Leif K., ',\n",
       " 'Cai, Xinlun, ',\n",
       " 'Ding, Yunhong, ']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0].authors_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "83e02b39-8a39-4561-aeae-82cc8b34b6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.883577\n",
       "1    0.000000\n",
       "2    0.000000\n",
       "3    0.088797\n",
       "4    0.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9bdf1616-c28f-4afb-88d4-9d239bb16b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46206</th>\n",
       "      <td>0.119213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.870843</td>\n",
       "      <td>Snegirev, A. V.,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0    1    2    3         4             author\n",
       "46206  0.119213  0.0  0.0  0.0  0.870843  Snegirev, A. V., "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = train_topics_authors.author == 'Snegirev, A. V., '\n",
    "train_topics_authors[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "da779f44-08f1-4ad7-b4e9-4095b2430ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "157208    False\n",
       "157209    False\n",
       "157210    False\n",
       "157211    False\n",
       "157212    False\n",
       "Name: author, Length: 157213, dtype: bool"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f010137-49b4-48f3-a2f0-39d1c7cb039b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
