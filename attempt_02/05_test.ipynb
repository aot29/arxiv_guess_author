{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97146dde-de0d-4fc3-8482-d195fd7a1c33",
   "metadata": {},
   "source": [
    "# Test\n",
    "* Load the tokenized abstracts of the test dataset.\n",
    "* Extract the topics of each article applying the topic model, load the article metadata and merge with the topics into one data frame. \n",
    "* Find the authors closest to the articles in the topic space.\n",
    "* Compute the probability that the closest author is one of the authors of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3ffccf1-c753-4e8a-81cc-547cd1087a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2081b5e4-a045-4cc7-a748-d51fd74bc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'\n",
    "MODELS_PATH = '../models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23784fe5-5c6c-4882-a901-ea4b91105bd9",
   "metadata": {},
   "source": [
    "## Load the tokenized abstracts of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c258dd1-ab89-48a2-9011-567d1b30a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary\n",
    "with open(os.path.join(DATA_PATH, 'dictionary.pickle'), 'rb') as handle:\n",
    "    dictionary = tokenized_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dacfd7f5-a081-49ce-ad89-e5eb8d7fbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized abstracts\n",
    "def load_tokenized_dataset(file_name):\n",
    "    path = os.path.join(DATA_PATH, file_name)\n",
    "    with open(path, 'rb') as handle:\n",
    "        tokenized_dataset = pickle.load(handle)\n",
    "    return tokenized_dataset\n",
    "\n",
    "corpus_test = load_tokenized_dataset(\"corpus_test.pickle\")\n",
    "corpus_validate = load_tokenized_dataset(\"corpus_validate.pickle\")\n",
    "corpus_train = load_tokenized_dataset(\"corpus_train.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690062b3-10ce-4cd2-a078-610b9b207583",
   "metadata": {},
   "source": [
    "## Extract the topics of each article applying the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3b4378f-fe62-4c24-82b6-664de2042d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the topic model\n",
    "with open(os.path.join(MODELS_PATH, 'topic_model.pickle'), 'rb') as handle:\n",
    "    topic_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e552f396-feb1-416e-a19b-b6acc95c9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(topic_model, corpus):\n",
    "    \"\"\"\n",
    "    Returns a list of pandas Series object of tuples. \n",
    "    Each tuple is a topic number and the topic probability for this entry in the corpus.\n",
    "    Example: \n",
    "        [[(0, 0.22764261), (4, 0.14444388), (5, 0.62411755)],\n",
    "         [(1, 0.024827635), (2, 0.3290665), (3, 0.6061594), (5, 0.03431195)],\n",
    "         [(0, 0.06239689), (3, 0.03924617), (5, 0.8926314)],\n",
    "         [(3, 0.09784623), (5, 0.89414346)],...\n",
    "        ...]\n",
    "    If for a given entry, the topic's probability is 0, then the topic is not included in the Series for this entry.\n",
    "    \"\"\"\n",
    "    topic_details_list = []\n",
    "    for row in topic_model[corpus]:\n",
    "        topic_details_list.append(row)\n",
    "    return topic_details_list\n",
    "\n",
    "def get_topic_dataframe(topic_model, corpus):\n",
    "    \"\"\"\n",
    "    Returns a data frame with a column for each topic in the topic model.\n",
    "    Each row stands for an entry in the corpus, each value for the probability of thos topic for this entry.\n",
    "    If for a given entry, the topic's probability is 0, the the value in the entry's column corresponding to the topic is also 0.\n",
    "    Example:\n",
    "             \t0 \t1 \t2 \t3 \t4 \t5\n",
    "        0 \t0.227641 \t0.000000 \t0.000000 \t0.000000 \t0.144445 \t0.624118\n",
    "        1 \t0.000000 \t0.024817 \t0.329062 \t0.606161 \t0.000000 \t0.034325\n",
    "        2 \t0.062392 \t0.000000 \t0.000000 \t0.039281 \t0.000000 \t0.892601\n",
    "        3 \t0.000000 \t0.000000 \t0.000000 \t0.097728 \t0.000000 \t0.894262\n",
    "    \"\"\"\n",
    "    topic_details = get_topic_details(topic_model, corpus)\n",
    "    topics_entries = []  # topics for all entries\n",
    "    num_topics = len(topic_model.get_topics())  # number of topics in the model\n",
    "    for row in topic_details:\n",
    "        topics_entry = [0] * num_topics\n",
    "        for entry in row:  # all topic probabilities for this entry\n",
    "            topic_num = entry[0]  # the topic number\n",
    "            topic_prob = entry[1]  # the topic probability            \n",
    "            topics_entry[topic_num] = topic_prob\n",
    "        topics_entries.append(topics_entry)\n",
    "    return pd.DataFrame(topics_entries, columns=range(0, num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35ec5a79-954b-4697-9929-19c7b8eb4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to assign topics probabilities to all articles in test data set\n",
    "topics_test_df = get_topic_dataframe(topic_model, corpus_test)\n",
    "# Use the model to assign topics probabilities to all articles in validate data set\n",
    "topics_validate_df = get_topic_dataframe(topic_model, corpus_validate)\n",
    "# Use the model to assign topics probabilities to all articles in train data set\n",
    "topics_train_df = get_topic_dataframe(topic_model, corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f6a2f-978e-4783-8525-f017be16260b",
   "metadata": {},
   "source": [
    "## Load the article metadata and merge with the topics into one data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f8b14-3820-49fd-8a8b-d5b7fedcf278",
   "metadata": {},
   "source": [
    "* load the article metadata\n",
    "* Add names to the topic columns, i.e. topic_0, ...\n",
    "* merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90157ff5-911f-4b55-8541-6bb5e4ca3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(name):\n",
    "    return pd.read_csv(\n",
    "        os.path.join(DATA_PATH, name), \n",
    "        index_col=0, \n",
    "        converters={\"authors_parsed\": lambda x:[entry.replace(\"\\'\", '').replace('\"', '').strip(\"[]\") for entry in x.split('\", \"')]})\n",
    "\n",
    "validate_df = load_df('arxiv_validate.csv')\n",
    "test_df = load_df('arxiv_test.csv')\n",
    "train_df = load_df('arxiv_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6afbff9b-b3c6-4cba-a798-80ad0e01d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(topic_model.get_topics())\n",
    "\n",
    "for l in [topics_test_df, topics_validate_df, topics_train_df]:\n",
    "        l.columns = [\"topic_%d\"%n for n in range(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9226f322-4409-4f42-b7d7-cd3222c48191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(article_df, topics_df):\n",
    "    merged = article_df.reset_index(drop=True).join(topics_df.reset_index(drop=True))\n",
    "    return merged\n",
    "\n",
    "test_df = merge_df(test_df, topics_test_df)\n",
    "validate_df = merge_df(validate_df, topics_validate_df)\n",
    "train_df = merge_df(train_df, topics_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfafce6-f671-4bf6-aa41-5487004709b6",
   "metadata": {},
   "source": [
    "## Find the authors closest to the articles in the topic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5185a76-c04e-49f1-b38d-5629978dd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the topics by author\n",
    "authors_validate_df = pd.read_csv(os.path.join(DATA_PATH, 'validate_topics_authors.csv'), index_col=0)\n",
    "authors_test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_topics_authors.csv'), index_col=0)\n",
    "authors_train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_topics_authors.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d992bde4-f100-4027-9c72-50cb3f94edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def topic_distance(topics1, topics2):\n",
    "    \"\"\"\n",
    "    topic1, topic2: numpy.ndarray, representing the probability that an article is about a topic. An article can have multiple topics.\n",
    "    Example:\n",
    "        0    0.000000\n",
    "        1    0.000000\n",
    "        2    0.000000\n",
    "        3    0.000000\n",
    "        4    0.000000\n",
    "        5    0.992668\n",
    "    topic1 and topic2 must be the same length.\n",
    "    Topics were assigned to articles in 03_assign_topics\n",
    "    \"\"\"\n",
    "    dist = np.linalg.norm(topics1 - topics2)  # euclidean distance, L2 norm is default\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f9d00-469f-4d5b-96a1-24f0296925ac",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Pick a random article from the validate data set, find the nearest author in validate dataset.\n",
    "Expect to guess one of the authors in the majority of cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "086bb61c-324e-44c5-988e-84b321c4e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_author(article, authors_df):\n",
    "    \"\"\"\n",
    "    Guess the author of article within authors_df.\n",
    "    \n",
    "    article: pandas.DataFrame data for a single article\n",
    "    authors_df: pandas.DataFrame data for all authors\n",
    "    \"\"\"\n",
    "    # numpy array with topics probabilities for the article\n",
    "    topics1 = np.array(article[[\"topic_%d\"%n for n in range(num_topics)]])\n",
    "    # numpy array of topics of all authors\n",
    "    topics_authors_np = authors_df[authors_df.columns.drop('author')].to_numpy() \n",
    "    # compute distances between article topics and topics of all authors\n",
    "    distances = [topic_distance(topics1, topics2) for topics2 in topics_authors_np]\n",
    "    # find the closest author\n",
    "    closest_author = authors_df.iloc[distances.index(min(distances))].author\n",
    "    return closest_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4f74af2e-a74a-4347-898b-562746cc3f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original article \"Revealing the spatial nature of sublattice symmetry\" authors are: ['Xiao, Rong, ', 'Zhao, Y. X., ']\n",
      "The closest author is: Xiao, Rong, \n"
     ]
    }
   ],
   "source": [
    "# pick a random paper\n",
    "rnd_article = validate_df.sample()\n",
    "closest_author = guess_author(rnd_article, authors_validate_df)\n",
    "\n",
    "# check\n",
    "print(f\"The original article \\\"{rnd_article.title.iloc[0]}\\\" authors are: {rnd_article.authors_parsed.iloc[0]}\")\n",
    "print(f\"The closest author is: {closest_author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9826ac39-a792-4881-8920-d41f7c55197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_author in rnd_article.authors_parsed.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0060938-137f-4ac9-8abf-0fa6b3e952ea",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "Pick a random article from the validate data set, by an author who is also in the train dataset. Find the nearest author in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8871659f-e6a0-485e-8177-af19f738da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validate dataset has 101576, the train dataset has 157213, and 50935 are in both datasets.\n"
     ]
    }
   ],
   "source": [
    "# Percentage of authors in validate dataset, that are also present in train dataset\n",
    "validate_authors = set(validate_topics_authors.author)\n",
    "train_authors = set(train_topics_authors.author)\n",
    "intersection_authors = validate_authors.intersection(train_authors)\n",
    "print(f\"The validate dataset has {len(validate_authors)}, the train dataset has {len(train_authors)}, and {len(intersection_authors)} are in both datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f66d0834-8c56-4910-8270-01443052ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19018 out of 22632 article in the validate dataset were written by authors present in the validate and the train datasets\n"
     ]
    }
   ],
   "source": [
    "# get papers in validate dataset, by authors in both datasets\n",
    "intersection_ids = []\n",
    "for i, row in validate_df.iterrows():\n",
    "    for author in row.authors_parsed:\n",
    "        # if at least one author is in both datasets, keep the article id\n",
    "        if author in intersection_authors:\n",
    "            intersection_ids.append(row.id)\n",
    "            break\n",
    "\n",
    "idx = [article_id in intersection_ids for article_id in validate_df.id]\n",
    "articles_intersection_df = validate_df[idx]\n",
    "\n",
    "print(f\"{articles_intersection_df.shape[0]} out of {validate_df.shape[0]} article in the validate dataset were written by authors present in the validate and the train datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "04db9080-d03e-443a-b45d-39e803b2bf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 μs\n",
      "Computing 157213 distances\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# pick a random paper from the intersection dataset\n",
    "topics_intersection_df = topics_validate_df[idx]\n",
    "rnd_article = topics_intersection_df.sample()\n",
    "# numpy array with topics probabilities for the article\n",
    "topics1 = np.array(rnd_article.values[0])\n",
    "\n",
    "print(f\"Computing {train_topics_authors.shape[0]} distances\")\n",
    "train_topics_authors_np = train_topics_authors.iloc[:, 0:5].to_numpy()  # the topic_authors dataframes have an extra column for author names\n",
    "distances = [topic_distance(topics1, topics2) for topics2 in train_topics_authors_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ea0e24bb-9389-4b58-b1b5-4a9fc3395212",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m original_article \u001b[38;5;241m=\u001b[39m \u001b[43marticles_intersection_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrnd_article\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe original article \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00moriginal_article\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m authors are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_article\u001b[38;5;241m.\u001b[39mauthors_parsed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/arxiv_exp/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/arxiv_exp/lib/python3.11/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/arxiv_exp/lib/python3.11/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "original_article = articles_intersection_df.iloc[rnd_article.index[0]]\n",
    "print(f\"The original article \\\"{original_article.title}\\\" authors are: {original_article.authors_parsed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "03c70073-d8b8-43db-af05-f237088c2292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19166"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_article.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "40269af3-5ca0-4bab-8572-d0f486be9621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19018, 26)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_intersection_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9c0691ba-c66a-4dba-b56f-6cb922fe7d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest author is: François, Matijn, \n"
     ]
    }
   ],
   "source": [
    "closest_author = train_topics_authors.iloc[distances.index(min(distances))]\n",
    "print(f\"The closest author is: {closest_author.author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f6d430ce-2192-47e5-a71f-02c312fca255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_author.author in original_article.authors_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06abf2-62ec-4388-8bfd-a09366e0b0be",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a41eb7b1-ec1b-40e8-9f63-fea0fe7576e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics for the authors in the train dataset\n",
    "# the topic_authors dataframes have an extra column for author names, so use the 5 first columns only\n",
    "topics_authors_np = train_topics_authors.iloc[:, 0:5].to_numpy()  \n",
    "\n",
    "def guess_author(article_topics):\n",
    "    # numpy array with topics probabilities for the article\n",
    "    topics1 = np.array(article)\n",
    "    # compute distance from article topics to all authors topics in the train dataset\n",
    "    distances = [topic_distance(topics1, topics2) for topics2 in topics_authors_np]\n",
    "    # find closest author\n",
    "    closest_author = train_topics_authors.iloc[distances.index(min(distances))]\n",
    "    return closest_author\n",
    "\n",
    "def guess_authors(article_topics_df, article_df):\n",
    "    closest_authors = []\n",
    "    for i, article in article_topics_df.iterrows():\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processing article {i+1}/{article_topics_df.shape[0]}\")\n",
    "        closest_authors.append(guess_author(article))\n",
    "    return closest_authors\n",
    "\n",
    "def check_guess(article_df, closest_authors):\n",
    "    for i, article in article_df.iterrows():\n",
    "        # check if correct\n",
    "        check.append(closest_author[i].author in article.authors_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "512998f9-d6a2-4d9b-a164-d2b98ba7217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 236 ms, sys: 23.8 ms, total: 260 ms\n",
      "Wall time: 245 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                  0.119213\n",
       "1                       0.0\n",
       "2                       0.0\n",
       "3                       0.0\n",
       "4                  0.870843\n",
       "author    Snegirev, A. V., \n",
       "Name: 46206, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "guess_author(topics_train_df.iloc[0])\n",
    "\n",
    "#guesses = guess_authors(topics_train_df, train_df)\n",
    "#check = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1e26bb7e-8722-467c-87d4-5dfe97afb68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ekici, Cagin, ',\n",
       " 'Yu, Yonghe, ',\n",
       " 'Adcock, Jeremy C., ',\n",
       " 'Muthali, Alif Laila, ',\n",
       " 'Zahidy, Mujtaba, ',\n",
       " 'Tan, Heyun, ',\n",
       " 'Lin, Zhongjin, ',\n",
       " 'Li, Hao, ',\n",
       " 'Oxenløwe, Leif K., ',\n",
       " 'Cai, Xinlun, ',\n",
       " 'Ding, Yunhong, ']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0].authors_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "83e02b39-8a39-4561-aeae-82cc8b34b6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.883577\n",
       "1    0.000000\n",
       "2    0.000000\n",
       "3    0.088797\n",
       "4    0.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9bdf1616-c28f-4afb-88d4-9d239bb16b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46206</th>\n",
       "      <td>0.119213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.870843</td>\n",
       "      <td>Snegirev, A. V.,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0    1    2    3         4             author\n",
       "46206  0.119213  0.0  0.0  0.0  0.870843  Snegirev, A. V., "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = train_topics_authors.author == 'Snegirev, A. V., '\n",
    "train_topics_authors[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "da779f44-08f1-4ad7-b4e9-4095b2430ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "157208    False\n",
       "157209    False\n",
       "157210    False\n",
       "157211    False\n",
       "157212    False\n",
       "Name: author, Length: 157213, dtype: bool"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f010137-49b4-48f3-a2f0-39d1c7cb039b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
