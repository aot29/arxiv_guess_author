{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e265b3ab-e23b-4539-ba14-a294b93a300a",
   "metadata": {},
   "source": [
    "# Filter, tokenize and split the data\n",
    "\n",
    "__Filter__\n",
    "* Load the data prepared in ../00_process_snapshot.ipynb\n",
    "* Filter by year and subject, count the number of authors\n",
    "\n",
    "__Split__\n",
    "* Split data into train/validate/test\n",
    "\n",
    "__Tokenize__\n",
    "* Apply pre-processing filters\n",
    "* Apply lemmatization\n",
    "* Save tokenized corpus, one for each train/validate/test dataset on NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce723da5-d0ca-450f-ad09-8951e912e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/kobv/atroncos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tok\n",
    "import zipfile as zf\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b80dee-341b-40d6-890c-6205279bc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e5251-2813-425c-9d3e-7357ee4ed59a",
   "metadata": {},
   "source": [
    "## Filter\n",
    "Load the data prepared in ../00_process_snapshot.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cac7873-6ef2-4943-ac69-5c8b5284e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_607853/2453297134.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  arxiv_df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "arxiv_df = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, 'arxiv_metadata.csv.zip'), \n",
    "    converters={\"authors_parsed\": lambda x:[entry.strip('[]') for entry in  x.split(\"], \")]}, \n",
    "    index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7805708-db96-4127-8c69-af7a7e617c44",
   "metadata": {},
   "source": [
    "* Filter by year: keep only articles submitted in the period considered\n",
    "* Filter by subject: choose 'Physics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b48da41-35f4-4701-9b21-b83d77f4e9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered data set has 90530 entries.\n"
     ]
    }
   ],
   "source": [
    "idx = arxiv_df['year'] >= 2023\n",
    "filtered_df = arxiv_df[idx]\n",
    "idx = filtered_df['Physics'] == True\n",
    "filtered_df = filtered_df[idx]\n",
    "print(f\"The filtered data set has {filtered_df.shape[0]} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba595424-8adf-4bba-bf46-52a7605fa700",
   "metadata": {},
   "source": [
    "Count authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ed8ded8-6a38-402e-93ec-ea0c94df0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    \"\"\"Flatten a list of lists\"\"\"\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def get_unique_authors(df):\n",
    "    \"\"\"Given a dataframe, return unique authors\"\"\"\n",
    "    authors = flatten(df['authors_parsed'])\n",
    "    return set(authors)\n",
    "\n",
    "def count_authors(df):\n",
    "    \"\"\"Given a dataframe, return count of unique authors\"\"\"\n",
    "    return len(get_unique_authors(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc5ece89-d2c0-4ad9-a17f-d0f7faed36cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered data set has 246443 unique authors.\n"
     ]
    }
   ],
   "source": [
    "# ALL AUTHORS\n",
    "count_all_authors = count_authors(filtered_df)\n",
    "print(f\"The filtered data set has {count_all_authors} unique authors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d4788-20e4-436b-9617-2437dd65b289",
   "metadata": {},
   "source": [
    "## Split data into train/validate/test\n",
    "\n",
    "\"train\"\n",
    "\n",
    "    A percent of the texts reserved for fitting the model.\n",
    "\n",
    "\"validate\"\n",
    "\n",
    "    A percent of the texts reserved for computing perplexity when fitting the model's k-parameter, and searching for best parameters.\n",
    "\n",
    "\"test\"\n",
    "\n",
    "    A percent of the texts reserved for testing hypotheses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d33174a5-74b3-4dfa-9973-f7039a75dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    train, test = train_test_split(df, test_size=0.5)\n",
    "    validate, test = train_test_split(test, test_size=0.5)\n",
    "    return(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b10c3c26-ab20-4f64-93e7-41736039bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset has 45265 rows, the validate dataset 22632 rows, the test dataset 22633 rows\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df, test_df = split(filtered_df)\n",
    "print(f\"The train dataset has {train_df.shape[0]} rows, the validate dataset {validate_df.shape[0]} rows, the test dataset {test_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163ad76-9e35-4857-8a4c-48ad4b9d39d3",
   "metadata": {},
   "source": [
    "## Save article data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "293a9d19-e506-4106-a119-f18859f6cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(DATA_PATH, 'arxiv_train.csv'))\n",
    "validate_df.to_csv(os.path.join(DATA_PATH, 'arxiv_validate.csv'))\n",
    "test_df.to_csv(os.path.join(DATA_PATH, 'arxiv_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ece1a-7081-49b4-9481-05f8fb47ec71",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Apply pre-processing filters: strip_tags, strip_punctuation, strip_multiple_whitespaces, stric_numeric, remove_stopwords; strip_short\n",
    "\n",
    "Apply lemmatization to the list of words.\n",
    "\n",
    "The tokenize functions are in the tok.py package provided in the same directory as the notebooks.\n",
    "\n",
    "see: https://github.com/piskvorky/gensim/blob/develop/gensim/parsing/preprocessing.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea23c77a-00a1-4f2e-9cef-3ffa8ba50c9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'abstract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/arxiv_exp/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abstract'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# make a dictionary with all the words in the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m texts \u001b[38;5;241m=\u001b[39m tok\u001b[38;5;241m.\u001b[39mclean(\u001b[43mfiltered_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m tok\u001b[38;5;241m.\u001b[39mmake_dictionary(texts)\n",
      "File \u001b[0;32m~/arxiv_exp/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/arxiv_exp/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abstract'"
     ]
    }
   ],
   "source": [
    "# make a dictionary with all the words in the dataset\n",
    "texts = tok.clean(filtered_df['abstract'])\n",
    "dictionary = tok.make_dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f57ce9-be49-4baf-b8fd-3850363aa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dictionary, df):\n",
    "    _texts = tok.clean(df['abstract'])\n",
    "    return(tok.make_corpus(dictionary, _texts))\n",
    "\n",
    "def tokenize_datasets(dictionary, train, validate, test):\n",
    "    corpus_train = tokenize_dataset(dictionary, train)\n",
    "    corpus_validate = tokenize_dataset(dictionary, validate)\n",
    "    corpus_test = tokenize_dataset(dictionary, test)\n",
    "    return(corpus_train, corpus_validate, corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6541e8-02a2-4e13-9693-851e314b314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train, validate and test data sets\n",
    "corpus_train, corpus_validate, corpus_test = tokenize_datasets(dictionary, train_df, validate_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f5b98-edbf-47ff-8954-53a5073727dd",
   "metadata": {},
   "source": [
    "## Save tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c846aa06-464a-4937-9875-bf1b1a161cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary\n",
    "with open(os.path.join(DATA_PATH, 'dictionary.pickle'), 'wb') as handle:\n",
    "    pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e4794a9-b3dc-469d-bf7e-9e9adc298602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenized_dataset(path, obj):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_tokenized_datasets(train, validate, test):\n",
    "    path_train = os.path.join(DATA_PATH, f\"corpus_train.pickle\")\n",
    "    save_tokenized_dataset(path_train, train)\n",
    "\n",
    "    path_validate = os.path.join(DATA_PATH, f\"corpus_validate.pickle\")\n",
    "    save_tokenized_dataset(path_validate, validate)\n",
    "\n",
    "    path_test = os.path.join(DATA_PATH, f\"corpus_test.pickle\")\n",
    "    save_tokenized_dataset(path_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a8e27e-bf7d-4802-9b0f-513cd4073176",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tokenized_datasets(corpus_train, corpus_validate, corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b060c-c350-4a32-8549-36231dcd4cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
