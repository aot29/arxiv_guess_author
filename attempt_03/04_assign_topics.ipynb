{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aece577-a110-4ec1-9709-dfa38654d44a",
   "metadata": {},
   "source": [
    "# Assign topic distributions to authors\n",
    "* List all authors in the dataset.\n",
    "* For each author, merge the abstracts of all their articles, apply pre-processing filters and lemmatization.\n",
    "* Finally, apply the trained LDA topic model to extract the topic distribution from each text and assign this topic distribution to each author. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e40e6fd-f41f-4bc9-907c-ef2af0cf3fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/kobv/atroncos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import arxiv_utils\n",
    "import tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f07ad5-1695-4dc6-92cd-5e1c231896a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'\n",
    "MODELS_PATH = '../models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc484a-986d-465a-a7c9-e25e7f475c6a",
   "metadata": {},
   "source": [
    "## Load the article abstracts and metadata for the train/validate/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c06833-f7ad-45af-9aa2-667416c214cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(name):\n",
    "    return pd.read_csv(\n",
    "        os.path.join(DATA_PATH, name), \n",
    "        index_col=0, \n",
    "        converters={\"authors_parsed\": lambda x:[entry.replace(\"\\'\", '').replace('\"', '').strip(\"[]\") for entry in x.split('\", \"')]})\n",
    "\n",
    "train_df = load_df('arxiv_train.csv')\n",
    "validate_df = load_df('arxiv_validate.csv')\n",
    "test_df = load_df('arxiv_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08085457-510b-4b82-98d0-966b53104f31",
   "metadata": {},
   "source": [
    "## List all authors in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f13928c-cafa-4d65-9611-fed2d31ae490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    \"\"\"Flatten a list of lists\"\"\"\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def get_unique_authors(df):\n",
    "    \"\"\"Given a dataframe, return unique authors\"\"\"\n",
    "    authors = flatten(df['authors_parsed'])\n",
    "    return set(authors)\n",
    "\n",
    "def count_authors(df):\n",
    "    \"\"\"Given a dataframe, return count of unique authors\"\"\"\n",
    "    return len(get_unique_authors(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27918bb4-5590-4a6e-a5e2-85f2e69e423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset has 157213 unique authors, the validate dataset 101576, the test dataset 99444.\n"
     ]
    }
   ],
   "source": [
    "train_authors = arxiv_utils.get_unique_authors(train_df)\n",
    "validate_authors = arxiv_utils.get_unique_authors(validate_df)\n",
    "test_authors = arxiv_utils.get_unique_authors(test_df)\n",
    "print(f\"The train dataset has {len(train_authors)} unique authors, the validate dataset {len(validate_authors)}, the test dataset {len(test_authors)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e91188-a10f-46af-a762-88d61f7bff0f",
   "metadata": {},
   "source": [
    "## For each author, merge the abstracts of all their articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1dc986-d13d-4553-919c-5f94af3c2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstracts_by_author(author, metadata_df):\n",
    "    \"\"\"\n",
    "    Get the abstracts of all articles by this author.\n",
    "    author: string\n",
    "    metadata_df: pandas.DataFrame dataset\n",
    "    returns: list<string>\n",
    "    \"\"\"\n",
    "    idx = [author in authors_parsed for authors_parsed in metadata_df['authors_parsed']]\n",
    "    abstracts = metadata_df[idx]['abstract']\n",
    "    return list(abstracts)\n",
    "\n",
    "def merge_abstracts_by_author(author_list, metadata_df):\n",
    "    \"\"\"\n",
    "    Merge the abstracts of all articles by all authors.\n",
    "    author_list: set<string> set of unique author names\n",
    "    metadata_df: pandas.DataFrame dataset\n",
    "    returns: list<string> merged abstracts\n",
    "    \"\"\"\n",
    "    txts = []\n",
    "    counter = 0\n",
    "    for author in author_list:\n",
    "        if counter % 25000 == 0:\n",
    "            print(f\"Processing author {counter}/{len(author_list)}\")\n",
    "        abstracts = get_abstracts_by_author(author, metadata_df)\n",
    "        txt = ' '.join(abstracts)\n",
    "        txts.append(txt)\n",
    "        counter += 1\n",
    "    return txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd86f916-1f21-4a22-85ce-bb2a160206ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset\n",
      "Processing author 0/157213\n",
      "Processing author 25000/157213\n",
      "Processing author 50000/157213\n",
      "Processing author 75000/157213\n",
      "Processing author 100000/157213\n",
      "Processing author 125000/157213\n",
      "Processing author 150000/157213\n",
      "Processing validate dataset\n",
      "Processing author 0/101576\n",
      "Processing author 25000/101576\n",
      "Processing author 50000/101576\n",
      "Processing author 75000/101576\n",
      "Processing author 100000/101576\n",
      "Processing test dataset\n",
      "Processing author 0/99444\n",
      "Processing author 25000/99444\n",
      "Processing author 50000/99444\n",
      "Processing author 75000/99444\n",
      "CPU times: user 18min 16s, sys: 975 ms, total: 18min 17s\n",
      "Wall time: 18min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Processing train dataset\")\n",
    "train_authors_txts = merge_abstracts_by_author(train_authors, train_df)\n",
    "print(\"Processing validate dataset\")\n",
    "validate_authors_txts = merge_abstracts_by_author(validate_authors, validate_df)\n",
    "print(\"Processing test dataset\")\n",
    "test_authors_txts = merge_abstracts_by_author(test_authors, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b016dc-5091-4f3b-b36a-a8f796a8a981",
   "metadata": {},
   "source": [
    "## Apply pre-processing filters and lemmatization to the texts for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724e51f8-0c88-4fa7-8a78-94d87558c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'dictionary.pickle'), 'rb') as handle:\n",
    "    dictionary = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f2a285-8a1b-4277-aa24-0f908c26d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset\n",
      "Tokenizing validate dataset\n",
      "Tokenizing test dataset\n",
      "CPU times: user 2min 45s, sys: 1.25 s, total: 2min 47s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def tokenize_dataset(dictionary, txt_list):\n",
    "    _texts = tok.clean(txt_list)\n",
    "    return(tok.make_corpus(dictionary, _texts))\n",
    "\n",
    "print(\"Tokenizing train dataset\")\n",
    "train_corpus_authors = tokenize_dataset(dictionary, train_authors_txts)\n",
    "print(\"Tokenizing validate dataset\")\n",
    "validate_corpus_authors = tokenize_dataset(dictionary, validate_authors_txts)\n",
    "print(\"Tokenizing test dataset\")\n",
    "test_corpus_authors = tokenize_dataset(dictionary, test_authors_txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb99c3-20e5-4801-b341-bbef0dad8935",
   "metadata": {},
   "source": [
    "## Apply the trained LDA topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de36c4f-45ec-4ba0-b33f-b326dd21b7a0",
   "metadata": {},
   "source": [
    "Load the topic model\n",
    "Load the LDA topic model fitted in [03_fit_topic_model](./03_fit_topic_model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f81d0dfb-895d-44f8-ba90-bdb81df05e7b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(MODELS_PATH, 'topic_model9.pickle'), 'rb') as handle:\n",
    "    topic_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706f0f2-b826-4f6b-af93-80d1bb1ee141",
   "metadata": {},
   "source": [
    "Extract the topic distribution from each text and assign this topic distribution to each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd32c211-6397-490f-912a-e6ef3c2a9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(topic_model, corpus):\n",
    "    \"\"\"\n",
    "    Returns a list of pandas Series object of tuples. \n",
    "    Each tuple is a topic number and the topic probability for this entry in the corpus.\n",
    "    Example: \n",
    "        [[(0, 0.22764261), (4, 0.14444388), (5, 0.62411755)],\n",
    "         [(1, 0.024827635), (2, 0.3290665), (3, 0.6061594), (5, 0.03431195)],\n",
    "         [(0, 0.06239689), (3, 0.03924617), (5, 0.8926314)],\n",
    "         [(3, 0.09784623), (5, 0.89414346)],...\n",
    "        ...]\n",
    "    If for a given entry, the topic's probability is 0, then the topic is not included in the Series for this entry.\n",
    "    \"\"\"\n",
    "    topic_details_list = []\n",
    "    for row in topic_model[corpus]:\n",
    "        topic_details_list.append(row)\n",
    "    return topic_details_list\n",
    "\n",
    "def get_topic_dataframe(topic_model, corpus):\n",
    "    \"\"\"\n",
    "    Returns a data frame with a column for each topic in the topic model.\n",
    "    Each row stands for an entry in the corpus, each value for the probability of thos topic for this entry.\n",
    "    If for a given entry, the topic's probability is 0, the the value in the entry's column corresponding to the topic is also 0.\n",
    "    Example:\n",
    "             \t0 \t1 \t2 \t3 \t4 \t5\n",
    "        0 \t0.227641 \t0.000000 \t0.000000 \t0.000000 \t0.144445 \t0.624118\n",
    "        1 \t0.000000 \t0.024817 \t0.329062 \t0.606161 \t0.000000 \t0.034325\n",
    "        2 \t0.062392 \t0.000000 \t0.000000 \t0.039281 \t0.000000 \t0.892601\n",
    "        3 \t0.000000 \t0.000000 \t0.000000 \t0.097728 \t0.000000 \t0.894262\n",
    "    \"\"\"\n",
    "    topic_details = get_topic_details(topic_model, corpus)\n",
    "    topics_entries = []  # topics for all entries\n",
    "    num_topics = len(topic_model.get_topics())  # number of topics in the model\n",
    "    for row in topic_details:\n",
    "        topics_entry = [0] * num_topics\n",
    "        for entry in row:  # all topic probabilities for this entry\n",
    "            topic_num = entry[0]  # the topic number\n",
    "            topic_prob = entry[1]  # the topic probability            \n",
    "            topics_entry[topic_num] = topic_prob\n",
    "        topics_entries.append(topics_entry)\n",
    "    return pd.DataFrame(topics_entries, columns=range(0, num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e81546a-0d7c-4205-b989-d243b7d98bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 1min 19s, total: 2min 34s\n",
      "Wall time: 59.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_topics_authors = get_topic_dataframe(topic_model, train_corpus_authors)\n",
    "validate_topics_authors = get_topic_dataframe(topic_model, validate_corpus_authors)\n",
    "test_topics_authors = get_topic_dataframe(topic_model, test_corpus_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c284b-90df-4090-b83d-d7e0bf68d539",
   "metadata": {},
   "source": [
    "## Add author names and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6008796-8edf-481e-8168-6367b759670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_topics_authors[\"author\"] = list(train_authors)\n",
    "validate_topics_authors[\"author\"] = list(validate_authors)\n",
    "test_topics_authors[\"author\"] = list(test_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f13ac15-19bb-4485-9aa0-39618e734666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_topics_authors.to_csv(os.path.join(DATA_PATH, 'train_topics9_authors.csv'))\n",
    "validate_topics_authors.to_csv(os.path.join(DATA_PATH, 'validate_topics9_authors.csv'))\n",
    "test_topics_authors.to_csv(os.path.join(DATA_PATH, 'test_topics9_authors.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5eecfa-0893-4c1f-9458-1696efe37ff2",
   "metadata": {},
   "source": [
    "# Assign topic distributions to articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7393bbfb-14bf-4938-9d4a-5bf1849a1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized abstracts\n",
    "def load_tokenized_dataset(file_name):\n",
    "    path = os.path.join(DATA_PATH, file_name)\n",
    "    with open(path, 'rb') as handle:\n",
    "        tokenized_dataset = pickle.load(handle)\n",
    "    return tokenized_dataset\n",
    "\n",
    "corpus_test = load_tokenized_dataset(\"corpus_test.pickle\")\n",
    "corpus_validate = load_tokenized_dataset(\"corpus_validate.pickle\")\n",
    "corpus_train = load_tokenized_dataset(\"corpus_train.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4478e97c-03c2-46a4-8656-658e718d4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to assign topics probabilities to all articles in test data set\n",
    "topics_test_df = get_topic_dataframe(topic_model, corpus_test)\n",
    "# Use the model to assign topics probabilities to all articles in validate data set\n",
    "topics_validate_df = get_topic_dataframe(topic_model, corpus_validate)\n",
    "# Use the model to assign topics probabilities to all articles in train data set\n",
    "topics_train_df = get_topic_dataframe(topic_model, corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881c61c-e4b1-4183-9db8-4483f9c72768",
   "metadata": {},
   "source": [
    "## Load the article metadata and merge with the topics into one data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8815b036-0ac0-4677-b69d-489d5b600cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(name):\n",
    "    return pd.read_csv(\n",
    "        os.path.join(DATA_PATH, name), \n",
    "        index_col=0, \n",
    "        converters={\"authors_parsed\": lambda x:[entry.replace(\"\\'\", '').replace('\"', '').strip(\"[]\") for entry in x.split('\", \"')]})\n",
    "\n",
    "validate_df = load_df('arxiv_validate.csv')\n",
    "test_df = load_df('arxiv_test.csv')\n",
    "train_df = load_df('arxiv_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fc3ec40-942d-46c8-9038-edfe5562dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(topic_model.get_topics())\n",
    "\n",
    "for l in [topics_test_df, topics_validate_df, topics_train_df]:\n",
    "        l.columns = [\"topic9_%d\"%n for n in range(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9eade6e-ab7a-4047-8d5b-0bd38ca3549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(article_df, topics_df):\n",
    "    merged = article_df.reset_index(drop=True).join(topics_df.reset_index(drop=True))\n",
    "    return merged\n",
    "\n",
    "test_df = merge_df(test_df, topics_test_df)\n",
    "validate_df = merge_df(validate_df, topics_validate_df)\n",
    "train_df = merge_df(train_df, topics_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e666e32-47e5-40da-8dbb-3baffacb8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(DATA_PATH, 'train_topics9.csv'))\n",
    "validate_df.to_csv(os.path.join(DATA_PATH, 'validate_topics9.csv'))\n",
    "test_df.to_csv(os.path.join(DATA_PATH, 'test_topics9.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13661a-5f1b-46a2-a7ed-8a8d2134060f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
