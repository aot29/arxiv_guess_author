{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e265b3ab-e23b-4539-ba14-a294b93a300a",
   "metadata": {},
   "source": [
    "# Filter, tokenize and split the data\n",
    "\n",
    "__Filter__\n",
    "* Load the data prepared in ../00_process_snapshot.ipynb\n",
    "* Filter by year and subject, count the number of authors\n",
    "\n",
    "__Split__\n",
    "* Split data into train/validate/test\n",
    "\n",
    "__Tokenize__\n",
    "* Apply pre-processing filters\n",
    "* Apply lemmatization\n",
    "* Save tokenized corpus, one for each train/validate/test dataset on NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce723da5-d0ca-450f-ad09-8951e912e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/kobv/atroncos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tok\n",
    "import zipfile as zf\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b80dee-341b-40d6-890c-6205279bc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e5251-2813-425c-9d3e-7357ee4ed59a",
   "metadata": {},
   "source": [
    "## Filter\n",
    "Load the data prepared in ../00_process_snapshot.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cac7873-6ef2-4943-ac69-5c8b5284e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3872207/758826194.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  arxiv_df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "arxiv_df = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, 'arxiv_metadata.csv'), \n",
    "    converters={\"authors_parsed\": lambda x:[entry.strip('[]') for entry in  x.split(\"], \")]}, \n",
    "    index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7805708-db96-4127-8c69-af7a7e617c44",
   "metadata": {},
   "source": [
    "* Filter by year: keep only articles submitted in the period considered\n",
    "* Filter by subject: choose 'Physics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b48da41-35f4-4701-9b21-b83d77f4e9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered data set has 90530 entries.\n"
     ]
    }
   ],
   "source": [
    "idx = arxiv_df['year'] >= 2023\n",
    "filtered_df = arxiv_df[idx]\n",
    "idx = filtered_df['Physics'] == True\n",
    "filtered_df = filtered_df[idx]\n",
    "print(f\"The filtered data set has {filtered_df.shape[0]} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba595424-8adf-4bba-bf46-52a7605fa700",
   "metadata": {},
   "source": [
    "Count authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed8ded8-6a38-402e-93ec-ea0c94df0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    \"\"\"Flatten a list of lists\"\"\"\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def get_unique_authors(df):\n",
    "    \"\"\"Given a dataframe, return unique authors\"\"\"\n",
    "    authors = flatten(df['authors_parsed'])\n",
    "    return set(authors)\n",
    "\n",
    "def count_authors(df):\n",
    "    \"\"\"Given a dataframe, return count of unique authors\"\"\"\n",
    "    return len(get_unique_authors(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc5ece89-d2c0-4ad9-a17f-d0f7faed36cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered data set has 246443 unique authors.\n"
     ]
    }
   ],
   "source": [
    "# ALL AUTHORS\n",
    "count_all_authors = count_authors(filtered_df)\n",
    "print(f\"The filtered data set has {count_all_authors} unique authors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d4788-20e4-436b-9617-2437dd65b289",
   "metadata": {},
   "source": [
    "## Split data into train/validate/test\n",
    "\n",
    "\"train\"\n",
    "\n",
    "    A percent of the texts reserved for fitting the model.\n",
    "\n",
    "\"validate\"\n",
    "\n",
    "    A percent of the texts reserved for computing perplexity when fitting the model's k-parameter, and searching for best parameters.\n",
    "\n",
    "\"test\"\n",
    "\n",
    "    A percent of the texts reserved for testing hypotheses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d33174a5-74b3-4dfa-9973-f7039a75dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    train, test = train_test_split(df, test_size=0.5)\n",
    "    validate, test = train_test_split(test, test_size=0.5)\n",
    "    return(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b10c3c26-ab20-4f64-93e7-41736039bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset has 45265 rows, the validate dataset 22632 rows, the test dataset 22633 rows\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df, test_df = split(filtered_df)\n",
    "print(f\"The train dataset has {train_df.shape[0]} rows, the validate dataset {validate_df.shape[0]} rows, the test dataset {test_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c5c0eef-0c91-42c6-b5b9-e391c66cdcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Hani', 'Zaher', ''\", \"'Shatah', 'Jalal', ''\", \"'Zhu', 'Hui', ''\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"authors_parsed\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163ad76-9e35-4857-8a4c-48ad4b9d39d3",
   "metadata": {},
   "source": [
    "## Save article data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293a9d19-e506-4106-a119-f18859f6cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(DATA_PATH, 'arxiv_train.csv'))\n",
    "validate_df.to_csv(os.path.join(DATA_PATH, 'arxiv_validate.csv'))\n",
    "test_df.to_csv(os.path.join(DATA_PATH, 'arxiv_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ece1a-7081-49b4-9481-05f8fb47ec71",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Apply pre-processing filters: strip_tags, strip_punctuation, strip_multiple_whitespaces, stric_numeric, remove_stopwords; strip_short\n",
    "\n",
    "Apply lemmatization to the list of words.\n",
    "\n",
    "The tokenize functions are in the tok.py package provided in the same directory as the notebooks.\n",
    "\n",
    "see: https://github.com/piskvorky/gensim/blob/develop/gensim/parsing/preprocessing.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea23c77a-00a1-4f2e-9cef-3ffa8ba50c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary with all the words in the dataset\n",
    "texts = tok.clean(filtered_df['abstract'])\n",
    "dictionary = tok.make_dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f57ce9-be49-4baf-b8fd-3850363aa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dictionary, df):\n",
    "    _texts = tok.clean(df['abstract'])\n",
    "    return(tok.make_corpus(dictionary, _texts))\n",
    "\n",
    "def tokenize_datasets(dictionary, train, validate, test):\n",
    "    corpus_train = tokenize_dataset(dictionary, train)\n",
    "    corpus_validate = tokenize_dataset(dictionary, validate)\n",
    "    corpus_test = tokenize_dataset(dictionary, test)\n",
    "    return(corpus_train, corpus_validate, corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6541e8-02a2-4e13-9693-851e314b314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train, validate and test data sets\n",
    "corpus_train, corpus_validate, corpus_test = tokenize_datasets(dictionary, train_df, validate_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f5b98-edbf-47ff-8954-53a5073727dd",
   "metadata": {},
   "source": [
    "## Save tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c846aa06-464a-4937-9875-bf1b1a161cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary\n",
    "with open(os.path.join(DATA_PATH, 'dictionary.pickle'), 'wb') as handle:\n",
    "    pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e4794a9-b3dc-469d-bf7e-9e9adc298602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenized_dataset(path, obj):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_tokenized_datasets(train, validate, test):\n",
    "    path_train = os.path.join(DATA_PATH, f\"corpus_train.pickle\")\n",
    "    save_tokenized_dataset(path_train, train)\n",
    "\n",
    "    path_validate = os.path.join(DATA_PATH, f\"corpus_validate.pickle\")\n",
    "    save_tokenized_dataset(path_validate, validate)\n",
    "\n",
    "    path_test = os.path.join(DATA_PATH, f\"corpus_test.pickle\")\n",
    "    save_tokenized_dataset(path_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a8e27e-bf7d-4802-9b0f-513cd4073176",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tokenized_datasets(corpus_train, corpus_validate, corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b060c-c350-4a32-8549-36231dcd4cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
